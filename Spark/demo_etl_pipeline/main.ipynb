{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported two libraries: **SparkSession** and **SQLContext**.\n",
    "\n",
    "SparkSession is the entry point for programming Spark applications. It lets us interact with DataSet and DataFrame APIs provided by Spark. We set the application name by calling appName. The getOrCreate() method either returns a new SparkSession of the app or returns an existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "scSpark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"reading_csv\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from CSV (**Extract**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next objective is to read CSV files. But before we move further, let’s play with some real data. For that purpose, we are using Supermarket’s sales data which I got from [Kaggle](https://www.kaggle.com/aungpyaeap/supermarket-sales). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/18 16:14:15 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "data_file = 'supermarket_sales.csv'\n",
    "sdfData = scSpark.read.csv(data_file, header=True, sep=\",\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdfData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating the Data (**Transform**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DataFrame methods on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+--------+--------+\n",
      "|Gender|     City|Unit price|Quantity|   Total|\n",
      "+------+---------+----------+--------+--------+\n",
      "|Female|   Yangon|     74.69|       7|548.9715|\n",
      "|  Male|   Yangon|     58.22|       8| 489.048|\n",
      "|Female|   Yangon|     68.84|       6| 433.692|\n",
      "|Female|   Yangon|     36.26|       2|  76.146|\n",
      "|Female| Mandalay|     54.84|       3| 172.746|\n",
      "|Female| Mandalay|     14.48|       4|  60.816|\n",
      "|  Male| Mandalay|     25.51|       4| 107.142|\n",
      "|Female| Mandalay|     93.72|       6| 590.436|\n",
      "|Female|   Yangon|     68.93|       7|506.6355|\n",
      "|  Male|Naypyitaw|     86.04|       5|  451.71|\n",
      "|  Male|   Yangon|     88.63|       3|279.1845|\n",
      "|Female|   Yangon|     52.59|       8| 441.756|\n",
      "|  Male| Mandalay|     78.07|       9|737.7615|\n",
      "|Female|Naypyitaw|     99.42|       4| 417.564|\n",
      "|Female|Naypyitaw|     68.12|       1|  71.526|\n",
      "|  Male|   Yangon|     62.62|       5| 328.755|\n",
      "|  Male| Mandalay|     30.12|       8| 253.008|\n",
      "|Female| Mandalay|     86.72|       1|  91.056|\n",
      "|  Male|Naypyitaw|     56.11|       2| 117.831|\n",
      "|Female| Mandalay|     69.12|       6| 435.456|\n",
      "+------+---------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdfData = sdfData.na.fill(value=\"NA\")\n",
    "sdfData = sdfData.withColumn(\n",
    "    \"Unit price after 1 year\",\n",
    "    sdfData[\"Unit price\"]+((sdfData[\"Unit price\"]/100)*6)\n",
    ")\n",
    "sdfData_filtered = sdfData.filter(\n",
    "    sdfData[\"Customer type\"] == 'Member'\n",
    "    ).select([\n",
    "        \"Gender\", \n",
    "        \"City\", \n",
    "        \"Unit price\", \n",
    "        \"Quantity\", \n",
    "        \"Total\"])\n",
    "sdfData_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running SQL Queries on the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a temporary table out of the dataframe. For that purpose **registerTampTable** is used. In our case the table name is sales. Once it’s done you can use typical SQL queries on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/subhayu/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "sdfData_filtered.registerTempTable(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+--------+--------+\n",
      "|Gender|     City|Unit price|Quantity|   Total|\n",
      "+------+---------+----------+--------+--------+\n",
      "|Female| Mandalay|     14.48|       4|  60.816|\n",
      "|Female|Naypyitaw|     12.54|       1|  13.167|\n",
      "|Female| Mandalay|     10.59|       3| 33.3585|\n",
      "|Female| Mandalay|     13.22|       5|  69.405|\n",
      "|Female|   Yangon|     14.23|       5| 74.7075|\n",
      "|  Male|Naypyitaw|      14.7|       5|  77.175|\n",
      "|  Male|Naypyitaw|     13.98|       1|  14.679|\n",
      "|  Male|   Yangon|     10.13|       7| 74.4555|\n",
      "|  Male|Naypyitaw|     12.05|       5| 63.2625|\n",
      "|  Male|   Yangon|     11.94|       3|  37.611|\n",
      "|Female|Naypyitaw|     10.53|       5| 55.2825|\n",
      "|Female| Mandalay|     12.29|       9|116.1405|\n",
      "|Female|   Yangon|     10.69|       5| 56.1225|\n",
      "|Female|Naypyitaw|     10.16|       5|   53.34|\n",
      "|  Male|Naypyitaw|     10.17|       1| 10.6785|\n",
      "|Female| Mandalay|     11.85|       8|   99.54|\n",
      "|Female|Naypyitaw|     14.87|       2|  31.227|\n",
      "|  Male|   Yangon|     12.76|       2|  26.796|\n",
      "|Female| Mandalay|      12.1|       8|  101.64|\n",
      "|Female|Naypyitaw|     10.18|       8|  85.512|\n",
      "+------+---------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = scSpark.sql('SELECT * from sales WHERE `Unit price` < 15 AND Quantity < 10')\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|total|     City|\n",
      "+-----+---------+\n",
      "|  169|Naypyitaw|\n",
      "|  165| Mandalay|\n",
      "|  167|   Yangon|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = scSpark.sql('SELECT COUNT(*) as total, City from sales GROUP BY City')\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Output (**Load**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the load part of the ETL. What if we want to save this transformed data? Well, we have many options available, RDBMS, XML or JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the below code block, Sparks will create a folder with the name of the file, in our case it is filtered.json. Then, a file with the name _SUCCESS, which tells whether the operation was a success or not. In case it fails a file with the name _FAILURE is generated. Then, we find multiple files here. The reason for multiple files is that each work is involved in the operation of writing in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write.save('filtered.json', 'json', 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a single file(which is not recommended) then **coalesce** can be used that collects and reduces the data from all partitions to a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.coalesce(3).write.format('json').save('filtered.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
