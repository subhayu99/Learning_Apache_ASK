{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/18 11:09:00 WARN Utils: Your hostname, FFT-ThinkPad-L490 resolves to a loopback address: 127.0.1.1; using 192.168.29.4 instead (on interface wlp5s0)\n",
      "22/08/18 11:09:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/18 11:09:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# a SparkSession object can perform the most common data processing tasks\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate() # will return existing session if one was\n",
    "                                                           # created before and was not closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f714ef53130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset:**\n",
    "https://www.kaggle.com/fedesoriano/heart-failure-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv, all columns will be of type string\n",
    "df = spark.read.option('header','true').csv('heart.csv')\n",
    "# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's\n",
    "# my favorite\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "# let PySpark infer the schema\n",
    "df = spark.read.csv('heart.csv', inferSchema=True, header=True, nullValue='NA')\n",
    "# save data\n",
    "df.write.format(\"csv\").save(\"heart_save.csv\")\n",
    "# if you want to overwrite the file\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| 40|  M|          ATA|      140|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "| 49|  F|          NAP|      160|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|\n",
      "| 37|  M|          ATA|      130|        283|        0|        ST|   98|             N|    0.0|      Up|           0|\n",
      "+---+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show head of table\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 40|\n",
      "| 49|\n",
      "| 37|\n",
      "+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---+---+\n",
      "|Age|Sex|\n",
      "+---+---+\n",
      "| 40|  M|\n",
      "| 49|  F|\n",
      "| 37|  M|\n",
      "+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show parts of the table\n",
    "df.select('Age').show(3)\n",
    "df.select(['Age','Sex']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PySpark DataFrame to Pandas DataFrame\n",
    "pd_df = df.toPandas()\n",
    "# convert it back\n",
    "spark_df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Age=40, Sex='M', ChestPainType='ATA', RestingBP=140, Cholesterol=289, FastingBS=0, RestingECG='Normal', MaxHR=172, ExerciseAngina='N', Oldpeak=0.0, ST_Slope='Up', HeartDisease=0),\n",
       " Row(Age=49, Sex='F', ChestPainType='NAP', RestingBP=160, Cholesterol=180, FastingBS=0, RestingECG='Normal', MaxHR=156, ExerciseAngina='N', Oldpeak=1.0, ST_Slope='Flat', HeartDisease=1),\n",
       " Row(Age=37, Sex='M', ChestPainType='ATA', RestingBP=130, Cholesterol=283, FastingBS=0, RestingECG='ST', MaxHR=98, ExerciseAngina='N', Oldpeak=0.0, ST_Slope='Up', HeartDisease=0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first three rows as three row objects, which is how spark represents single rows from a table.\n",
    "# we will learn more about it later\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- ChestPainType: string (nullable = true)\n",
      " |-- RestingBP: integer (nullable = true)\n",
      " |-- Cholesterol: integer (nullable = true)\n",
      " |-- FastingBS: integer (nullable = true)\n",
      " |-- RestingECG: string (nullable = true)\n",
      " |-- MaxHR: integer (nullable = true)\n",
      " |-- ExerciseAngina: string (nullable = true)\n",
      " |-- Oldpeak: double (nullable = true)\n",
      " |-- ST_Slope: string (nullable = true)\n",
      " |-- HeartDisease: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# type os columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 'int'),\n",
       " ('Sex', 'string'),\n",
       " ('ChestPainType', 'string'),\n",
       " ('RestingBP', 'int'),\n",
       " ('Cholesterol', 'int'),\n",
       " ('FastingBS', 'int'),\n",
       " ('RestingECG', 'string'),\n",
       " ('MaxHR', 'int'),\n",
       " ('ExerciseAngina', 'string'),\n",
       " ('Oldpeak', 'double'),\n",
       " ('ST_Slope', 'string'),\n",
       " ('HeartDisease', 'int')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column dtypes as list of tuples\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast a column from one type to other\n",
    "from pyspark.sql.types import FloatType\n",
    "df = df.withColumn(\"Age\",df.Age.cast(FloatType()))\n",
    "df = df.withColumn(\"RestingBP\",df.Age.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|               Age|         RestingBP|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               918|               918|\n",
      "|   mean|53.510893246187365|53.510893246187365|\n",
      "| stddev|  9.43261650673202|  9.43261650673202|\n",
      "|    min|              28.0|              28.0|\n",
      "|    max|              77.0|              77.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute summery statistics\n",
    "df.select(['Age','RestingBP']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column or replace existing one\n",
    "AgeFixed = df['Age'] + 1  # select alwayes returns a DataFrame object, and we need a column object\n",
    "df = df.withColumn('AgeFixed', AgeFixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|          AgeFixed|               Age|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               918|               918|\n",
      "|   mean|54.510893246187365|53.510893246187365|\n",
      "| stddev|  9.43261650673202|  9.43261650673202|\n",
      "|    min|              29.0|              28.0|\n",
      "|    max|              78.0|              77.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['AgeFixed','Age']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "| Age|Sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|\n",
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "|40.0|  M|          ATA|     40.0|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|\n",
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove columns\n",
    "df.drop('AgeFixed').show(1) # add df = to get the new DataFrame into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|40.0|\n",
      "+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename a column\n",
    "df.withColumnRenamed('Age','age').select('age').show(1)\n",
    "# to rename more than a single column, i would suggest a loop.\n",
    "name_pairs = [('Age','age'),('Sex','sex')]\n",
    "for old_name, new_name in name_pairs:\n",
    "    df = df.withColumnRenamed(old_name,new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| age|sex|\n",
      "+----+---+\n",
      "|40.0|  M|\n",
      "+----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','sex']).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that contain any NA\n",
    "df = df.na.drop()\n",
    "df.count()\n",
    "# drop all rows where all values are NA\n",
    "df = df.na.drop(how='all')\n",
    "# drop all rows where more at least 2 values are NOT NA\n",
    "df = df.na.drop(thresh=2)\n",
    "# drop all rows where any value at specific columns are NAs.\n",
    "df = df.na.drop(how='any', subset=['age','sex']) # 'any' is the defult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values in a specific column with a '?'\n",
    "df = df.na.fill(value='?',subset=['sex'])\n",
    "# replace NAs with mean of column\n",
    "from pyspark.ml.feature import Imputer # In statistics, imputation is the process of\n",
    "                                       # replacing missing data with substituted values\n",
    "imptr = Imputer(inputCols=['age','RestingBP'],\n",
    "                outputCols=['age','RestingBP']).setStrategy('mean') # can also be 'median' and so on\n",
    "\n",
    "df = imptr.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: float, sex: string, ChestPainType: string, RestingBP: float, Cholesterol: int, FastingBS: int, RestingECG: string, MaxHR: int, ExerciseAngina: string, Oldpeak: double, ST_Slope: string, HeartDisease: int, AgeFixed: float]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter to adults only and calculate mean\n",
    "df.filter('age > 18')\n",
    "df.where('age > 18')# 'where' is an alias to 'filter'\n",
    "df.where(df['age'] > 18) # third option\n",
    "# add another condition ('&' means and, '|' means or)\n",
    "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n",
    "# take every record where the 'ChestPainType' is NOT 'ATA'\n",
    "df.filter(~(df['ChestPainType'] == 'ATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+--------+\n",
      "| age|sex|ChestPainType|RestingBP|Cholesterol|FastingBS|RestingECG|MaxHR|ExerciseAngina|Oldpeak|ST_Slope|HeartDisease|AgeFixed|\n",
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+--------+\n",
      "|40.0|  M|          ATA|     40.0|        289|        0|    Normal|  172|             N|    0.0|      Up|           0|    41.0|\n",
      "|49.0|  F|          NAP|     49.0|        180|        0|    Normal|  156|             N|    1.0|    Flat|           1|    50.0|\n",
      "|37.0|  M|          ATA|     37.0|        283|        0|        ST|   98|             N|    0.0|      Up|           0|    38.0|\n",
      "|48.0|  F|          ASY|     48.0|        214|        0|    Normal|  108|             Y|    1.5|    Flat|           1|    49.0|\n",
      "|54.0|  M|          NAP|     54.0|        195|        0|    Normal|  122|             N|    0.0|      Up|           0|    55.0|\n",
      "|39.0|  M|          NAP|     39.0|        339|        0|    Normal|  170|             N|    0.0|      Up|           0|    40.0|\n",
      "|45.0|  F|          ATA|     45.0|        237|        0|    Normal|  170|             N|    0.0|      Up|           0|    46.0|\n",
      "|54.0|  M|          ATA|     54.0|        208|        0|    Normal|  142|             N|    0.0|      Up|           0|    55.0|\n",
      "|37.0|  M|          ASY|     37.0|        207|        0|    Normal|  130|             Y|    1.5|    Flat|           1|    38.0|\n",
      "|48.0|  F|          ATA|     48.0|        284|        0|    Normal|  120|             N|    0.0|      Up|           0|    49.0|\n",
      "|37.0|  F|          NAP|     37.0|        211|        0|    Normal|  142|             N|    0.0|      Up|           0|    38.0|\n",
      "|58.0|  M|          ATA|     58.0|        164|        0|        ST|   99|             Y|    2.0|    Flat|           1|    59.0|\n",
      "|39.0|  M|          ATA|     39.0|        204|        0|    Normal|  145|             N|    0.0|      Up|           0|    40.0|\n",
      "|49.0|  M|          ASY|     49.0|        234|        0|    Normal|  140|             Y|    1.0|    Flat|           1|    50.0|\n",
      "|42.0|  F|          NAP|     42.0|        211|        0|        ST|  137|             N|    0.0|      Up|           0|    43.0|\n",
      "|54.0|  F|          ATA|     54.0|        273|        0|    Normal|  150|             N|    1.5|    Flat|           0|    55.0|\n",
      "|38.0|  M|          ASY|     38.0|        196|        0|    Normal|  166|             N|    0.0|    Flat|           1|    39.0|\n",
      "|43.0|  F|          ATA|     43.0|        201|        0|    Normal|  165|             N|    0.0|      Up|           0|    44.0|\n",
      "|60.0|  M|          ASY|     60.0|        248|        0|    Normal|  125|             N|    1.0|    Flat|           1|    61.0|\n",
      "|36.0|  M|          ATA|     36.0|        267|        0|    Normal|  160|             N|    3.0|    Flat|           1|    37.0|\n",
      "+----+---+-------------+---------+-----------+---------+----------+-----+--------------+-------+--------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('age > 18').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|new_col|\n",
      "+-------+\n",
      "|   48.2|\n",
      "|   59.0|\n",
      "|   44.6|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate a string expression into command\n",
    "from pyspark.sql.functions import expr\n",
    "exp = 'age + 0.2 * AgeFixed'\n",
    "df.withColumn('new_col', expr(exp)).select('new_col').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| age| avg(HeartDisease)|\n",
      "+----+------------------+\n",
      "|77.0|               1.0|\n",
      "|76.0|               0.5|\n",
      "|75.0|0.6666666666666666|\n",
      "|74.0|0.7142857142857143|\n",
      "|73.0|               1.0|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by age\n",
    "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
    "# sort values in desnding order\n",
    "from pyspark.sql.functions import desc\n",
    "disease_by_age.orderBy(desc(\"age\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| age| avg(HeartDisease)|\n",
      "+----+------------------+\n",
      "|77.0|               1.0|\n",
      "|76.0|               0.5|\n",
      "|75.0|0.6666666666666666|\n",
      "+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "disease_by_age = df.groupby('age').mean().select(['age','avg(HeartDisease)'])\n",
    "disease_by_age.orderBy(desc(\"age\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------------------+\n",
      "|min(age)|max(age)|          avg(age)|\n",
      "+--------+--------+------------------+\n",
      "|    28.0|    77.0|53.510893246187365|\n",
      "+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate to get several statistics for several columns\n",
    "# the available aggregate functions are avg, max, min, sum, count\n",
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df['age']),F.max(df['age']),F.avg(df['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------------+\n",
      "|HeartDisease|min(age)|          avg(age)|\n",
      "+------------+--------+------------------+\n",
      "|           1|    31.0|  55.8996062992126|\n",
      "|           0|    28.0|50.551219512195125|\n",
      "+------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('HeartDisease').agg(F.min(df['age']),F.avg(df['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "| age|sex|\n",
      "+----+---+\n",
      "|40.0|  M|\n",
      "|49.0|  F|\n",
      "+----+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+----+\n",
      "|older| age|\n",
      "+-----+----+\n",
      "|false|40.0|\n",
      "| true|49.0|\n",
      "+-----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run an SQL query on the data\n",
    "df.createOrReplaceTempView(\"df\") # tell PySpark how the table will be called in the SQL query\n",
    "spark.sql(\"\"\"SELECT age, sex from df\"\"\").show(2)\n",
    "\n",
    "# we also choose columns using SQL sytnx, with a command that combins '.select()' and '.sql()'\n",
    "df.selectExpr(\"age > 40 as older\", \"age\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+\n",
      "| age|  M|  F|\n",
      "+----+---+---+\n",
      "|64.0| 16|  6|\n",
      "|47.0| 15|  4|\n",
      "|58.0| 35|  7|\n",
      "+----+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "|sex|true|false|\n",
      "+---+----+-----+\n",
      "|  F| 174|   19|\n",
      "|  M| 664|   61|\n",
      "+---+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pivot - expensive operation\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n",
    "                    .pivot(\"older\", (\"true\", \"false\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+\n",
      "| age|MaxHR|Cholesterol|\n",
      "+----+-----+-----------+\n",
      "|40.0|  172|        289|\n",
      "|49.0|  156|        180|\n",
      "|37.0|   98|        283|\n",
      "|48.0|  108|        214|\n",
      "+----+-----+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','MaxHR','Cholesterol']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----+\n",
      "| age|Cholesterol|        Fvec|MaxHR|\n",
      "+----+-----------+------------+-----+\n",
      "|40.0|        289|[40.0,289.0]|  172|\n",
      "|49.0|        180|[49.0,180.0]|  156|\n",
      "|37.0|        283|[37.0,283.0]|   98|\n",
      "+----+-----------+------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# devide dataset to training features and target\n",
    "X_column_names = ['age','Cholesterol']\n",
    "target_colum_name = ['MaxHR']\n",
    "\n",
    "# convert feature columns into a columns where the vlues are feature vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "v_asmblr = VectorAssembler(inputCols=X_column_names, outputCol='Fvec')\n",
    "df = v_asmblr.transform(df)\n",
    "X = df.select(['age','Cholesterol','Fvec','MaxHR'])\n",
    "X.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide dataset into training and testing sets\n",
    "trainset, testset = X.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/18 11:09:19 WARN Instrumentation: [85ec5e28] regParam is zero, which might cause numerical instability and overfitting.\n",
      "[-1.0137974732829853,0.03994738325453204]\n",
      "183.47488201613427\n"
     ]
    }
   ],
   "source": [
    "# predict 'RestingBP' using linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
    "model = model.fit(trainset)\n",
    "print(model.coefficients)\n",
    "print(model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+-----+------------------+\n",
      "| age|Cholesterol|        Fvec|MaxHR|        prediction|\n",
      "+----+-----------+------------+-----+------------------+\n",
      "|29.0|        204|[29.0,204.0]|  202|162.22402147485224|\n",
      "|29.0|        243|[29.0,243.0]|  160|  163.781969421779|\n",
      "|30.0|        237|[30.0,237.0]|  170| 162.5284876489688|\n",
      "|31.0|        219|[31.0,219.0]|  150|160.79563727710425|\n",
      "|32.0|        254|[32.0,254.0]|  155| 161.1799982177299|\n",
      "|35.0|        198|[35.0,198.0]|  130|155.90155233562714|\n",
      "|37.0|        215|[37.0,215.0]|  170| 154.5530629043882|\n",
      "|37.0|        240|[37.0,240.0]|  165|155.55174748575152|\n",
      "|38.0|          0|  [38.0,0.0]|  120|144.95057803138081|\n",
      "|38.0|        289|[38.0,289.0]|  105|156.49537179194058|\n",
      "|39.0|        219|[39.0,219.0]|  140|152.68525749084037|\n",
      "|39.0|        241|[39.0,241.0]|  146|153.56409992244005|\n",
      "|40.0|          0|  [40.0,0.0]|  144|142.92298308481486|\n",
      "|40.0|        167|[40.0,167.0]|  114|149.59419608832172|\n",
      "|40.0|        235|[40.0,235.0]|  188|152.31061814962987|\n",
      "|40.0|        275|[40.0,275.0]|  150|153.90851347981118|\n",
      "|41.0|          0|  [41.0,0.0]|  111|141.90918561153188|\n",
      "|41.0|        214|[41.0,214.0]|  168|150.45792562800173|\n",
      "|41.0|        235|[41.0,235.0]|  153| 151.2968206763469|\n",
      "|41.0|        237|[41.0,237.0]|  138|151.37671544285598|\n",
      "+----+-----------+------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "model.evaluate(testset).predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|ChestPainTypeInxed|\n",
      "+------------------+\n",
      "|               2.0|\n",
      "|               1.0|\n",
      "|               2.0|\n",
      "+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# handel categorical features with ordinal indexing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\n",
    "indxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
